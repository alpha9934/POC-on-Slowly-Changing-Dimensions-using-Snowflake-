{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e481e20",
   "metadata": {
    "id": "30147e2a"
   },
   "source": [
    "# <font color=blue><center>Slowly Changing Dimensions in Snowflake Using Streams and Tasks</center></font>\n",
    "## Agenda\n",
    "\n",
    "### Snowflake Datawarehouse\n",
    "One of the broadest uses of Snowflake is building a data warehouse platform or enhancing the existing data lake. It offers all sorts of services to build an efficient Data warehouse with ETL capability and support for various external data partners.\n",
    "- Dimensions\n",
    "- Facts\n",
    "- SCD\n",
    "- Type-1\n",
    "- Type-2\n",
    "- Type-3\n",
    "\n",
    "### Architecture\n",
    "- Overview of data flow\n",
    "- Tech Stack\n",
    "- End result\n",
    "\n",
    "### Environment Setup\n",
    "- AWS EC2 instance and security group creation\n",
    "- Docker installation and running\n",
    "- Usage of docker-composer and starting all the tools\n",
    "- How to access tools in local machine\n",
    "\n",
    "### Amazon S3 Set up\n",
    "- S3 terminology\n",
    "- Accesskey creation\n",
    "- Bucket creatio\n",
    "- Demonstration to upload a file\n",
    "\n",
    "### Test data preparation\n",
    "- Faker\n",
    "- Code walkthru\n",
    "\n",
    "### Extraction\n",
    "- NiFi-S3 integration\n",
    "- Push files using NiFi\n",
    "\n",
    "### Snowflake Components\n",
    "- Warehouse\n",
    "- Database and Schema\n",
    "- Table\n",
    "- View\n",
    "- Stored procedure\n",
    "- Snow Pipe\n",
    "- Stream\n",
    "- Task\n",
    "\n",
    "### SCD Type-1 Implementation\n",
    "\n",
    "### SCD Type-2 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5f6a4c",
   "metadata": {},
   "source": [
    "## <font color=blue>Datawarehouse and Snowflake</font>\n",
    "\n",
    "### Data Warehouse\n",
    "![alt text](what-is-a-data-warehouse.png)\n",
    "\n",
    "### Snowflake Architecture\n",
    "![alt text ><](architecture-overview.png)\n",
    "\n",
    "### Dimensions and Facts\n",
    "![alt text ><](facts_dimensions.png)\n",
    "\n",
    "### SCD\n",
    "### Type-1\n",
    "### Type-2\n",
    "### Type-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4400bc9d",
   "metadata": {
    "id": "e7eaf504"
   },
   "source": [
    "## <font color=blue>Architecture</font>\n",
    "### Overview of data flow\n",
    "#### Data Flow Architecture\n",
    "![alt text](implementation_of_slowly_changing_dimensions_using_snowflake_services.png)\n",
    "### Tech Stack\n",
    "* AWS EC2 and S3\n",
    "* Docker\n",
    "* Jupyter Lab\n",
    "* NiFi\n",
    "* Python\n",
    "* Snowflake\n",
    "\n",
    "### End result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd2404a",
   "metadata": {
    "id": "bd740e6f"
   },
   "source": [
    "## <font color=blue>Environment Setup</font>\n",
    "### AWS EC2 instance and security group creation\n",
    "- t2.xlarge instance\n",
    "- 32GB of storage recommended\n",
    "- Allow ports 4000 - 38888\n",
    "- Connect to ec2 via ssh\n",
    " <code>ssh -i \"D:\\path\\to\\private\\key.pem\" user@Public_DNS</code>\n",
    " <br/>Example:<code>ssh -i \"D:\\Users\\pyerravelly\\Desktop\\twitter_analysis.pem\" ec2-user@ec2-54-203-235-65.us-west-2.compute.amazonaws.com</code><br/>\n",
    "- Port forwarding \n",
    " <code>ssh -i \"D:\\path\\to\\private\\key.pem\" user@Public_DNS</code>\n",
    " <br/>Example:<code>ssh -i \"D:\\Users\\pyerravelly\\Desktop\\twitter_analysis.pem\" ec2-user@ec2-34-208-254-29.us-west-2.compute.amazonaws.com -L 2081:localhost:2041 -L 4888:localhost:4888 -L 2080:localhost:2080 -L 8050:localhost:8050 -L 4141:localhost:4141</code><br/>\n",
    "- Copy from local to ec2\n",
    "  <code>scp -r -i \"D:\\Users\\pyerravelly\\Desktop\\twitter_analysis.pem\"</code>\n",
    "  <br/>Example:<code>scp -r -i \"D:\\Users\\pyerravelly\\Desktop\\twitter_analysis.pem\" D:\\Users\\pyerravelly\\Downloads\\spark-standalone-cluster-on-docker-master\\build\\docker\\docker-exp ec2-user@ec2-34-208-254-29.us-west-2.compute.amazonaws.com:/home/ec2-user/docker_exp\n",
    "</code>\n",
    "\n",
    "### Docker installation and running\n",
    "    \n",
    "### Usage of docker-composer and starting all the tools\n",
    "\n",
    "- Commands to install Docker\n",
    "\n",
    "<code>sudo yum update -y</code>\n",
    "<code><br/>sudo yum install docker</code>\n",
    "<code><br/>sudo curl -L \"https://github.com/docker/compose/releases/download/1.29.1/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose</code>\n",
    "<code><br/>sudo chmod +x /usr/local/bin/docker-compose</code>\n",
    "<code><br/>sudo gpasswd -a $USER docker</code>\n",
    "<code><br/>newgrp docker</code>\n",
    "<br/>Start Docker: <code>sudo systemctl start docker</code>\n",
    "<br/>Stop Docker: <code>sudo systemctl stop docker</code>\n",
    "\n",
    "- How to access tools in local machine <br/>\n",
    "    List Docker containers running: <code>docker ps</code><br/>\n",
    "    CLI access in Docker container: <code>docker exec -i -t nifi bash</code><br/>\n",
    "    Jupyter Lab at: http://localhost:4888/lab? <br/>\n",
    "    NiFi at: http://localhost:2080/nifi/ <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69e14e3",
   "metadata": {},
   "source": [
    "## <font color=blue>Amazon S3 Set up</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab14b719",
   "metadata": {},
   "source": [
    "### S3 terminology\n",
    "- Identity and Access Management (IAM)\n",
    "- Access Keys\n",
    "- Bucket\n",
    "- Folder\n",
    "\n",
    "### Accesskey creation\n",
    "### Bucket creation\n",
    "### Demonstration to upload a file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869fe0fc",
   "metadata": {
    "id": "6523b815"
   },
   "source": [
    "## <font color=blue>Extraction</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7fe942",
   "metadata": {
    "id": "c85d6007"
   },
   "source": [
    "### Test data preparation\n",
    "- Faker\n",
    "- Code walkthru\n",
    "- <code>docker exec -i -t nifi bash\n",
    "  mkdir -p scd \n",
    "  cp /opt/workspace/nifi/FakeDataset/customer_(timestamp).csv scd/\n",
    "  Example: \n",
    "    cp /opt/workspace/nifi/FakeDataset/customer_20210808141848.csv scd/\n",
    "  </code>\n",
    "\n",
    "### Nifi\n",
    "- Processor\n",
    "- Connection\n",
    "- Goto http://localhost:2080/nifi/\n",
    "- NiFi-S3 integration\n",
    "- Push files using NiFi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9d721e",
   "metadata": {
    "id": "402a9a13"
   },
   "source": [
    "## <font color=blue>Transformation and Load</font>\n",
    "\n",
    "### Snowflake Components\n",
    "- Warehouse/Virtual Warehouse\n",
    "- Database and Schema\n",
    "- Table\n",
    "- View\n",
    "- Stored procedure\n",
    "- Snow Pipe\n",
    "- Stream\n",
    "- Task\n",
    "\n",
    "#### SQS Setup\n",
    "\n",
    "### SCD Type-1 Implementation\n",
    "\n",
    "### SCD Type-2 Implementation"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "presentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
